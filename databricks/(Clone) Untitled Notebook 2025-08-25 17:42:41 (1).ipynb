{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4388ba72-d6a6-4693-963f-ccccf5609918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "base_url = \"abfss://mycontainer@adlsgen2salesdata2025.dfs.core.windows.net/salesdata\"\n",
    "\n",
    "datasets = {\n",
    "    \"products\": f\"{base_url}/Products/{today}/Products_{today}.csv\",\n",
    "    \"sales\": f\"{base_url}/Sales/{today}/Sales_{today}.csv\",\n",
    "    \"inventory\": f\"{base_url}/Inventory/{today}/Inventory_{today}.csv\",\n",
    "    \"status\": f\"{base_url}/Status/{today}/Status_{today}.csv\",\n",
    "}\n",
    "\n",
    "catalog = \"my_sales_catalog\"\n",
    "schema = \"sales_schema\"\n",
    "\n",
    "for table_name, file_path in datasets.items():\n",
    "    df = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(file_path))\n",
    "    \n",
    "    (df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{table_name}\"))\n",
    "    \n",
    "    print(f\"Table {table_name} updated for current {today} date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd69546a-5433-48e5-bbe7-c2209805f63a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "base_url = \"abfss://mycontainer@adlsgen2salesdata2025.dfs.core.windows.net/salesdata\"\n",
    "\n",
    "datasets = {\n",
    "    \"products\": f\"{base_url}/Products/{today}/Products_{today}.csv\",\n",
    "    \"sales\": f\"{base_url}/Sales/{today}/Sales_{today}.csv\",\n",
    "    \"inventory\": f\"{base_url}/Inventory/{today}/Inventory_{today}.csv\",\n",
    "    \"status\": f\"{base_url}/Status/{today}/Status_{today}.csv\",\n",
    "\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for table_name, file_path in datasets.items():\n",
    "    df = (spark.read\n",
    "            .option(\"header\", True)\n",
    "            .option(\"inferSchema\", True)\n",
    "            .csv(file_path))\n",
    "\n",
    "    dataframes[f\"{table_name}_df\"] = df\n",
    "\n",
    "    print(f\" DataFrame for {table_name} created.\")\n",
    "\n",
    "products_df = dataframes['products_df']\n",
    "sales_df = dataframes['sales_df']\n",
    "inventory_df = dataframes['inventory_df']\n",
    "status_df = dataframes['status_df']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba05af3-f5b1-4e11-9464-8dfa10f4eba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08964a14-cb45-4c90-b8e4-cf5894b18dcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "sales_df = sales_df.withColumn(\"timestamp\", to_date(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
    "sales_df = sales_df.withColumnRenamed(\"Timestamp\", \"Date\")\n",
    "sales_df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdb1a58-d5ad-4c5f-be64-880741ad08f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(sales_df)\n",
    "print(products_df)\n",
    "print(inventory_df)\n",
    "print(status_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9ac16f-e5b8-4f30-bee1-1cd397e0ed62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s = sales_df.alias(\"s\")\n",
    "p = products_df.alias(\"p\")\n",
    "st = status_df.alias(\"st\")\n",
    "\n",
    "joined_sales_df = (s.join(p, s.Product == p.ProductID)\n",
    "                  .join(st, s.Status == st.StatusID))\n",
    "\n",
    "joined_sales_df=joined_sales_df.select(\"s.OrderID\", \"s.FirstName\", \"s.LastName\", \"s.Country\", \"s.Quantity\", \"p.ProductName\", \"s.Price\", \"s.Date\", \"st.Status\")\n",
    "\n",
    "joined_sales_df.show(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8175a062-d16d-458b-bb4a-d02a55728fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "open_orders_df = joined_sales_df.filter(col(\"Status\") == \"Open\")\n",
    "\n",
    "\n",
    "open_orders_df = open_orders_df.groupby(\"Status\").agg(count(\"Status\").alias(\"TotalOpenOrders\"))\n",
    "\n",
    "open_orders_df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fda1353-f0a8-4e1e-9856-5d0ea54bc812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "sales_summary = sales_df.groupBy(\"Product\").agg(count(\"Quantity\").alias(\"NumberofSales\"),(sum(\"quantity\").alias(\"TotalQuantity\")))\n",
    "sales_summary.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14383671-8c29-48ac-a1a5-eab4cd79a29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ss = sales_summary.alias(\"ss\")\n",
    "p = products_df.alias(\"p\")\n",
    "i = inventory_df.alias(\"i\")\n",
    "\n",
    "upd_inv_df = (ss.join(i, ss.Product == i.ProductID, \"inner\")\n",
    "              .join(p, ss.Product == p.ProductID, \"inner\"))\n",
    "\n",
    "upd_inv_df = upd_inv_df.select(\"ss.Product\", \"ss.TotalQuantity\", \"i.InStock\", \"p.ProductName\")\n",
    "upd_inv_df = upd_inv_df.withColumn(\"UpdatedStock\", col(\"InStock\") - col(\"TotalQuantity\"))\n",
    "upd_inv_df = upd_inv_df.orderBy(\"Product\")\n",
    "upd_inv_df.show(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e573a1a5-91fe-4140-8852-b3ec78f1c2d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_sales_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"my_sales_catalog.sales_schema.joined_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a81a191-1a1f-4a06-8b46-1ecaf202700e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_sales_df.write.mode(\"append\").parquet(\"abfss://mycontainer@adlsgen2salesdata2025.dfs.core.windows.net/salesdata/cleaned/sales_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aeb09d5-4054-446c-8b7f-eb5a22e7061a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "upd_inv_df.write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"abfss://mycontainer@adlsgen2salesdata2025.dfs.core.windows.net/salesdata/cleaned/inventory\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Untitled Notebook 2025-08-25 17:42:41 (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
